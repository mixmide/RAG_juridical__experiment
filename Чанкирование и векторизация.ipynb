{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Чанкирование и векторизация\n",
        "### Данный этап выполняется в Google Colab с T4. Следует также добавить путь к документам из базы знаний в DRIVE_PATH и FOLDER_NAME."
      ],
      "metadata": {
        "id": "iS-hH_IymrA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFKNZt5s24z2",
        "outputId": "affbf3d6-7159-4716-907a-3a325d6320a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Установка необходимых пакетов (в Colab)\n",
        "!pip install -q langchain faiss-cpu sentence-transformers langchain-community\n",
        "\n",
        "# Монтируем Google Drive для доступа к файлам\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примечание: В каждом текстовом файле базы знаний в первой строке писалось название документа, после чего делался отступ в 3 пустые строки."
      ],
      "metadata": {
        "id": "qjrTzDcLKm4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Чанкирование и векторизация:**"
      ],
      "metadata": {
        "id": "UIR3s2XILC5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Указываем путь к нужной папке в Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive'\n",
        "FOLDER_NAME = 'FOLDER_NAME'\n",
        "DOCS_FOLDER = f\"{DRIVE_PATH}/{FOLDER_NAME}\"\n",
        "\n",
        "print(f\"Проверка файлов в папке: {DOCS_FOLDER}\")\n",
        "!ls \"{DOCS_FOLDER}\"  # Проверяем содержимое папки\n",
        "\n",
        "# Функция для извлечения названия документа\n",
        "def extract_document_title(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    title = \"\"\n",
        "    empty_line_count = 0\n",
        "    for line in lines:\n",
        "        if line.strip() == \"\":\n",
        "            empty_line_count += 1\n",
        "            if empty_line_count == 3:\n",
        "                break\n",
        "        else:\n",
        "            title += line.strip() + \" \"\n",
        "            empty_line_count = 0\n",
        "    return title.strip()\n",
        "\n",
        "# Функция для чанкирования текста\n",
        "def chunk_text(file_path, chunk_size=900, chunk_overlap=250):\n",
        "    # Извлекаем название документа\n",
        "    doc_title = extract_document_title(file_path)\n",
        "    # Читаем весь текст файла\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    # Удаляем название и все до трех пустых строк\n",
        "    text = text.split('\\n\\n\\n', 1)[1] if '\\n\\n\\n' in text else text\n",
        "    # Настраиваем сплиттер\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    # Разбиваем текст на чанки\n",
        "    chunks = splitter.split_text(text)\n",
        "    # Формируем список чанков с метаданными\n",
        "    chunk_documents = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_documents.append({\n",
        "            \"text\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"document\": doc_title,\n",
        "                \"chunk_number\": i + 1\n",
        "            }\n",
        "        })\n",
        "    return chunk_documents\n",
        "\n",
        "# Загрузка всех чанков из папки\n",
        "def load_all_chunks(docs_folder):\n",
        "    import os\n",
        "    all_chunks = []\n",
        "    for filename in os.listdir(docs_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(docs_folder, filename)\n",
        "            print(f\"Обработка: {filename}\")\n",
        "            chunks = chunk_text(file_path)\n",
        "            all_chunks.extend(chunks)\n",
        "    return all_chunks\n",
        "\n",
        "# Инициализация модели эмбеддингов\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-large\",\n",
        "    model_kwargs={'device': 'cuda'}  # Используем GPU Colab\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Создание векторной базы\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Загружаем все чанки\n",
        "all_chunks = load_all_chunks(DOCS_FOLDER)\n",
        "\n",
        "# Извлекаем текст и метаданные\n",
        "texts = [chunk['text'] for chunk in all_chunks]\n",
        "metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
        "\n",
        "# Создаем объекты Document для LangChain\n",
        "documents = []\n",
        "for i, (text, metadata) in enumerate(zip(texts, metadatas)):\n",
        "    documents.append(Document(\n",
        "        page_content=text,\n",
        "        metadata=metadata\n",
        "    ))\n",
        "\n",
        "# Генерируем эмбеддинги\n",
        "print(\"Генерация эмбеддингов...\")\n",
        "vectors = embedding_model.embed_documents(texts)\n",
        "\n",
        "# Нормализуем для косинусного сходства\n",
        "vectors = np.array(vectors)\n",
        "vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "\n",
        "# Создаем FAISS индекс\n",
        "dimension = vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Добавляем векторы пакетами\n",
        "batch_size = 50\n",
        "for i in tqdm(range(0, len(vectors), batch_size), desc=\"Создание индекса\"):\n",
        "    batch = vectors[i:i + batch_size]\n",
        "    index.add(batch)\n",
        "\n",
        "# Создаем ID для каждого документа\n",
        "ids = [str(i) for i in range(len(documents))]\n",
        "# Создаем InMemoryDocstore\n",
        "docstore = InMemoryDocstore(dict(zip(ids, documents)))\n",
        "# Создаем индекс для соответствия ID\n",
        "index_to_id = {i: str(i) for i in range(len(documents))}\n",
        "\n",
        "# инициализация FAISS\n",
        "db = FAISS(\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_id,\n",
        "    embedding_function=embedding_model.embed_query\n",
        ")\n",
        "\n",
        "# Сохраняем в Google Drive\n",
        "SAVE_PATH = f\"{DRIVE_PATH}/faiss_index_cosine\"\n",
        "db.save_local(SAVE_PATH)\n",
        "print(f\"\\nИндекс сохранен в: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "V-0ZKfXL4im2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Реализация функции поиска:**"
      ],
      "metadata": {
        "id": "NPzAxs0xK6Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция поиска\n",
        "def search_with_cosine_similarity(query, db, top_k=3, cosine_threshold=0.7):\n",
        "    # Получаем вектор запроса\n",
        "    query_embedding = db.embedding_function(query)\n",
        "    # Нормализуем вектор запроса\n",
        "    query_embedding = np.array(query_embedding) / np.linalg.norm(query_embedding)\n",
        "    # Выполняем поиск с использованием similarity_search_with_score для получения L2-дистанции\n",
        "    results = db.similarity_search_with_score(query, k=top_k)\n",
        "    print(\"\\nНайденные релевантные чанки:\")\n",
        "    for i, (doc, l2_distance) in enumerate(results, 1):\n",
        "        cosine_sim = 1 - (l2_distance ** 2) / 2\n",
        "        print(f\"\\nЧанк {i} (сходство: {cosine_sim:.4f}):\")\n",
        "        print(f\"Источник: {doc.metadata['document']}\")\n",
        "        print(f\"Текст: {doc.page_content[:200]}...\")\n",
        "\n",
        "    if results and (1 - (results[0][1]**2)/2) > cosine_threshold:\n",
        "        return [res[0] for res in results]\n",
        "    return None"
      ],
      "metadata": {
        "id": "lGIhlftP5pzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Тестирование поиска:**"
      ],
      "metadata": {
        "id": "eG62USP7K8-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример поиска\n",
        "query = \"Кто может быть залогодателем\"\n",
        "print(f\"\\nПоиск по запросу: '{query}'\")\n",
        "context = search_with_cosine_similarity(query, db)\n",
        "\n",
        "if context:\n",
        "    print(\"\\nКонтекст для ответа:\")\n",
        "    for i, doc in enumerate(context, 1):\n",
        "        print(f\"{i}. {doc.page_content[:150]}...\")\n",
        "else:\n",
        "    print(\"Релевантные документы не найдены\")"
      ],
      "metadata": {
        "id": "1WAdUc6uK2jw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}